1ï¸âƒ£ What is Generative AI?

Generative AI: models that create new content (text, images, code, audio) based on learned patterns.

Powered by deep neural networks (esp. Transformers).

ğŸ“Œ Examples:

ChatGPT â†’ generates text/code.

Stable Diffusion â†’ generates images.

ğŸ‘‰ Why it matters for your role?
Because enterprises (like Morgan Stanley, Wells Fargo â€“ Torana clients) want data-driven AI assistants that can retrieve, reason, and respond accurately.

2ï¸âƒ£ Large Language Models (LLMs)

Built on Transformers (decoder or encoder-decoder).

Trained on huge text corpora (web, books, code).

Learn next-word prediction â†’ but generalize to summarization, translation, reasoning.

ğŸ“Œ Examples:

GPT-3.5, GPT-4 â†’ OpenAI

LLaMA, Mistral â†’ Open-source

BERT â†’ encoder, for classification/search

3ï¸âƒ£ Fine-Tuning vs. Prompt Engineering
Fine-Tuning

Modify weights of model with new dataset.

Heavy compute, large data needed.

Example: Fine-tuning GPT on medical texts â†’ medical assistant.

Prompt Engineering

No retraining â†’ use clever prompts to guide model output.

Cheaper, faster.

Techniques:

Zero-shot prompting â†’ â€œTranslate this sentence to French.â€

Few-shot prompting â†’ give examples in prompt.

Chain-of-Thought prompting â†’ ask model to reason step by step.

ğŸ“Œ Example:
Instead of â†’ â€œWhat is 27 Ã— 42?â€
Better â†’ â€œSolve step by step: what is 27 Ã— 42?â€
â†’ Leads to more accurate reasoning.

4ï¸âƒ£ Embeddings & Vector Databases

LLMs are powerful, but they donâ€™t know your company data.

Solution: Represent documents as vectors (embeddings).

ğŸ‘‰ Example:
Sentence â†’ â€œAI helps data quality.â€
Embedding â†’ [0.12, -0.33, 0.87, â€¦] (dense vector).

Store embeddings in Vector DB (like FAISS, Pinecone, Chroma).

For search:

Convert query â†’ embedding.

Find nearest vectors (cosine similarity).

Retrieve relevant documents.

5ï¸âƒ£ Retrieval-Augmented Generation (RAG)

RAG = Retrieval + Generation

Pipeline:

User Query (â€œSummarize Pfizerâ€™s Q2 reportâ€).

Convert query â†’ embedding.

Search in Vector DB (find relevant documents).

Add retrieved context to query.

Pass enriched query to LLM â†’ get accurate answer.

ğŸ‘‰ Benefits:

LLMs answer with fresh, specific company data.

Avoids hallucinations.

Easier & cheaper than fine-tuning.

ğŸ“Œ Example (LangChain RAG):

from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
from langchain.vectorstores import Chroma

llm = OpenAI()
qa = RetrievalQA.from_chain_type(
    llm=llm, retriever=my_vector_db.as_retriever()
)

result = qa.run("Summarize Pfizerâ€™s Q2 report")
print(result)

6ï¸âƒ£ AI Agents

An AI Agent = LLM + tools + memory.

Instead of only answering, it can:

Search docs, run SQL queries, call APIs.

Plan multi-step reasoning.

Examples:

LangChain Agents

AutoGPT, BabyAGI

ğŸ“Œ Example Use-case at Torana:
An AI Agent that automatically:

Monitors ETL pipeline logs

Detects anomalies

Generates a report for engineers

ğŸ“ Generative AI Recap MCQs

Q1. What is the main advantage of RAG over fine-tuning?
âœ… RAG uses external knowledge dynamically without retraining.

Q2. Which database type is used for embeddings?
âœ… Vector Database (FAISS, Pinecone, Chroma)

Q3. In Prompt Engineering, what is â€œFew-shot learningâ€?
âœ… Giving model examples in the prompt.

Q4. Which mechanism powers LLMs like GPT?
âœ… Transformer (Self-Attention)
