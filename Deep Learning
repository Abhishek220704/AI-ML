1ï¸âƒ£ Neural Networks

A neural network is inspired by the human brain.

Composed of neurons arranged in layers:

Input layer â†’ Hidden layers â†’ Output layer.

Each neuron applies:

z=âˆ‘(wiâ‹…xi)+b

(activationÂ function)
y=f(z)(activationÂ function)

ğŸ‘‰ Training uses Gradient Descent:

Forward pass â†’ compute loss â†’ Backpropagation â†’ update weights.

ğŸ“Œ Example: Predicting if a student passes based on study hours + sleep.

2ï¸âƒ£ Activation Functions

Sigmoid: squashes between 0 and 1 â†’ used in binary classification.

Tanh: squashes between -1 and 1.

ReLU (Rectified Linear Unit):

f(x)=max(0,x)

Most common in hidden layers.

Softmax: converts outputs into probabilities (multi-class).

ğŸ‘‰ MCQ Example:
Which activation is most used in hidden layers of deep networks?
âœ… ReLU

3ï¸âƒ£ CNN (Convolutional Neural Networks)

Designed for images.

Convolution â†’ applies filters (detect edges, shapes, textures).

Pooling â†’ reduces dimensionality (max-pooling).

Fully Connected layers â†’ final classification.

ğŸ“Œ Example: Classifying MNIST digits.

ğŸ‘‰ Interview Q:
Why CNNs are better than fully connected networks for images?
âœ… Because they share weights and exploit spatial locality.

4ï¸âƒ£ RNN (Recurrent Neural Networks)

Designed for sequences (time-series, text).

Each stepâ€™s output depends on previous hidden state.

Problem â†’ Vanishing Gradient (canâ€™t capture long dependencies).

LSTM/GRU â†’ solve this by using gates (forget, input, output).

ğŸ“Œ Example: Predicting next word in a sentence.

5ï¸âƒ£ Transformers (Backbone of GPT, BERT, LLaMA)

Revolutionized NLP â†’ no recurrence, only Attention mechanism.

Self-Attention â†’ lets model focus on relevant words in a sequence.
Example: In â€œThe cat sat on the mat,â€ when predicting â€œmat,â€ the model attends to â€œsat on the.â€

Positional Encoding â†’ adds order information.

Encoder-Decoder architecture:

BERT â†’ only encoder (good for understanding tasks).

GPT â†’ only decoder (good for generation).

ğŸ“Œ Example: ChatGPT uses a decoder-only Transformer.

ğŸ‘‰ MCQ Example:
Which mechanism allows Transformers to capture long-range dependencies?
âœ… Self-Attention

ğŸ› ï¸ Mini Example (PyTorch)
import torch
import torch.nn as nn

# Simple feedforward NN
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(10, 20)  # 10 input features â†’ 20 hidden
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(20, 1)   # output layer

    def forward(self, x):
        x = self.relu(self.fc1(x))
        return self.fc2(x)

model = SimpleNN()
print(model)

ğŸ§  1ï¸âƒ£ Neural Networks â€“ Intuition & Math

A neural network is just a function approximator.

Each neuron takes inputs, multiplies with weights, adds bias, applies activation.

Equation for a single neuron:

whereÂ fÂ =Â activationÂ function
y=f(z)whereÂ fÂ =Â activationÂ function

ğŸ“Œ Example: Predicting if a student passes exam based on hours studied and hours slept.

ğŸ”„ 2ï¸âƒ£ Training a Neural Network

Steps:

Forward Pass â€“ compute predictions.

Loss Function â€“ measure error.

Regression â†’ MSE

Classification â†’ Cross-Entropy Loss

Backpropagation â€“ compute gradients of weights.

Optimizer (e.g., Gradient Descent, Adam) â€“ update weights.

ğŸ‘‰ Formula for weight update (Gradient Descent):

w:=wâˆ’Î·â‹…âˆ‚w/âˆ‚L

where Î· = learning rate.

ğŸ“Œ If Î· is too high â†’ diverges, too low â†’ very slow learning.

âš¡ 3ï¸âƒ£ Activation Functions

Sigmoid: 

Ïƒ(x)=1+eâˆ’x1
	â€‹
 â†’ good for probability output.
âŒ Problem: vanishing gradients.

Tanh: outputs (-1,1). Better than sigmoid, but still suffers vanishing gradient.

ReLU: 
f(x)=max(0,x) â†’ fast, solves vanishing gradient.
âŒ Problem: â€œdying ReLUâ€ (neuron stuck at 0).

Leaky ReLU: allows small slope for negatives.

Softmax: turns logits into probabilities for multi-class classification.

ğŸ“Œ Rule of thumb:

Hidden layers â†’ ReLU (or Leaky ReLU).

Binary output â†’ Sigmoid.

Multi-class output â†’ Softmax.

ğŸ–¼ï¸ 4ï¸âƒ£ CNN (Convolutional Neural Networks)

Best for image data.

Convolution Layer: filter (kernel) slides over image, extracts features (edges, textures).

Pooling Layer: reduces size (e.g., max pooling keeps strongest signal).

Fully Connected Layer: makes final prediction.

ğŸ‘‰ Why CNN is better than fully connected NN for images?
âœ… Because CNN shares weights (filter reused everywhere) + captures spatial patterns.

ğŸ“Œ Example: CNN for MNIST digit classification.

â³ 5ï¸âƒ£ RNNs & LSTMs

Used for sequential data (text, time-series, speech).

RNN: Each output depends on previous hidden state.
Problem â†’ vanishing gradient for long sequences.

LSTM (Long Short-Term Memory): fixes this using gates:

Forget gate â†’ what to keep/drop.

Input gate â†’ what to add.

Output gate â†’ what to output.

ğŸ‘‰ Example: Predict next word in a sentence.

ğŸ“Œ LSTM advantage â†’ remembers long-term dependencies.

ğŸ”¥ 6ï¸âƒ£ Transformers

Foundation of modern LLMs (GPT, BERT, LLaMA).

Key idea: Self-Attention.

Every word looks at all other words and assigns weights.

Captures long-range dependencies better than RNNs.

Equation (simplified self-attention):

Attention(Q,K,V)=softmax(dkQKT)V

Q = Query, K = Key, V = Value (all derived from word embeddings).

Positional Encoding adds order info.

ğŸ“Œ Architectures:

BERT â†’ encoder-only (good for understanding tasks: classification, QA).

GPT â†’ decoder-only (good for generation: text, code, chat).

ğŸ‘‰ Example: ChatGPT = stacked Transformer Decoders.

ğŸ“ Deep Learning Recap MCQs

Q1. Which activation is most widely used in hidden layers?
a) Sigmoid
b) Tanh
c) ReLU
d) Softmax
âœ… Answer: c) ReLU

Q2. Which problem does LSTM solve compared to vanilla RNN?
âœ… Vanishing gradient

Q3. Which mechanism allows Transformers to capture long-range dependencies?
âœ… Self-Attention

Q4. Why is pooling used in CNN?
âœ… To reduce dimensionality and extract dominant features.
