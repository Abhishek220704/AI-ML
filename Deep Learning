1ï¸âƒ£ Neural Networks

A neural network is inspired by the human brain.

Composed of neurons arranged in layers:

Input layer â†’ Hidden layers â†’ Output layer.

Each neuron applies:

z=âˆ‘(wiâ‹…xi)+b

(activationÂ function)
y=f(z)(activationÂ function)

ğŸ‘‰ Training uses Gradient Descent:

Forward pass â†’ compute loss â†’ Backpropagation â†’ update weights.

ğŸ“Œ Example: Predicting if a student passes based on study hours + sleep.

2ï¸âƒ£ Activation Functions

Sigmoid: squashes between 0 and 1 â†’ used in binary classification.

Tanh: squashes between -1 and 1.

ReLU (Rectified Linear Unit):

f(x)=max(0,x)

Most common in hidden layers.

Softmax: converts outputs into probabilities (multi-class).

ğŸ‘‰ MCQ Example:
Which activation is most used in hidden layers of deep networks?
âœ… ReLU

3ï¸âƒ£ CNN (Convolutional Neural Networks)

Designed for images.

Convolution â†’ applies filters (detect edges, shapes, textures).

Pooling â†’ reduces dimensionality (max-pooling).

Fully Connected layers â†’ final classification.

ğŸ“Œ Example: Classifying MNIST digits.

ğŸ‘‰ Interview Q:
Why CNNs are better than fully connected networks for images?
âœ… Because they share weights and exploit spatial locality.

4ï¸âƒ£ RNN (Recurrent Neural Networks)

Designed for sequences (time-series, text).

Each stepâ€™s output depends on previous hidden state.

Problem â†’ Vanishing Gradient (canâ€™t capture long dependencies).

LSTM/GRU â†’ solve this by using gates (forget, input, output).

ğŸ“Œ Example: Predicting next word in a sentence.

5ï¸âƒ£ Transformers (Backbone of GPT, BERT, LLaMA)

Revolutionized NLP â†’ no recurrence, only Attention mechanism.

Self-Attention â†’ lets model focus on relevant words in a sequence.
Example: In â€œThe cat sat on the mat,â€ when predicting â€œmat,â€ the model attends to â€œsat on the.â€

Positional Encoding â†’ adds order information.

Encoder-Decoder architecture:

BERT â†’ only encoder (good for understanding tasks).

GPT â†’ only decoder (good for generation).

ğŸ“Œ Example: ChatGPT uses a decoder-only Transformer.

ğŸ‘‰ MCQ Example:
Which mechanism allows Transformers to capture long-range dependencies?
âœ… Self-Attention

ğŸ› ï¸ Mini Example (PyTorch)
import torch
import torch.nn as nn

# Simple feedforward NN
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(10, 20)  # 10 input features â†’ 20 hidden
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(20, 1)   # output layer

    def forward(self, x):
        x = self.relu(self.fc1(x))
        return self.fc2(x)

model = SimpleNN()
print(model)
